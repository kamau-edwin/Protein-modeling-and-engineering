{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START ORIENTING MYSELF WITH NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COVER\n",
    "# PROCESSING AND UNDERSTANDING TEXT\n",
    "    #web scraping, data wrangling pre-processing\n",
    "# FEATURE ENGINEERING AND TEXT REPRESENTATION \n",
    "    # speech tagging, parsing, constituency and depency parsing\n",
    "    # named entity recognition\n",
    "# ANALYSIS--EMOTIONAL AND SENTIMENT ANALYSIS\n",
    " #supervised and unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA--inshort.com[tech,sports,world news]\n",
    "# CRISP-DM--Cross Industry Standard Process for Data Mining\n",
    "# Text Docs\n",
    "# Text pre-processing\n",
    "# Text Parsing & Exploratory Data Analysis\n",
    "# Text representation and Feature Engineering\n",
    "# Modeling and pattern mining\n",
    "# Evaluation Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#nltk.download() # downloaded first time \n",
    "#import urllib.request\n",
    "#response=urllib.request.urlopen('https://en.wikipedia.org/wiki/SpaceX')\n",
    "#html=response.read()\n",
    "#print(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that generates datasets from urls\n",
    "def build_data(seed_urls): \n",
    "    '''\n",
    "    This functions takes url or urls as input and generates\n",
    "    a dataframe of the headline, headline article\n",
    "    categorized by news category\n",
    "    '''\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    news_data=[]\n",
    "    news_columns = ['news_headline','news_article','news_category']\n",
    "    for url in seed_urls:\n",
    "        news_cat=url.split('/')[-1] # define category from url\n",
    "        data=requests.get(url) # get url \n",
    "        soup=BeautifulSoup(data.content,'html.parser')# parse html\n",
    "        # generate a dictionary comprehension \n",
    "        news_articles = [{'news_headline': headline.find('span', \n",
    "                    attrs={\"itemprop\": \"headline\"}).string,\n",
    "                    'news_article': article.find('div', \n",
    "                    attrs={\"itemprop\": \"articleBody\"}).string,\n",
    "                    'news_category': news_cat}\n",
    "                    for headline, article in zip(soup.find_all('div', \n",
    "                                               class_=[\"news-card-title news-right-box\"]),\n",
    "                                 soup.find_all('div', class_=[\"news-card-content news-right-box\"]))\n",
    "                        ]\n",
    "        news_data.extend(news_articles)\n",
    "    df = pd.DataFrame(news_data)[news_columns]\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN function to get todays news by category \n",
    "seed_urls = ['https://inshorts.com/en/read/technology',\n",
    "             'https://inshorts.com/en/read/sports',\n",
    "             'https://inshorts.com/en/read/world'] # \n",
    "today=build_data(seed_urls) # todays news "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "technology    25\n",
       "sports        25\n",
       "world         24\n",
       "Name: news_category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exploratory data analysis\n",
    "today.news_category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT WRANGLING AND PRE-PROCESSING\n",
    "    # removing HTML tags\n",
    "    # expanding contractions\n",
    "    # removing accented and special characters  \n",
    "# need installation of spacy\n",
    " # install with pip3 -U spacy\n",
    " # python -m spacy download en # download language mode\n",
    "   # english,german,spanish\n",
    " # restart the kennel if after above it does not work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(corpus,strip_html=True,accent=True,special_char_removal=True,\n",
    "    remove_digits=True,expansion=True,lower_case=True,\n",
    "    stemmer=False,lemmatize=True,stopword=True):\n",
    "    '''\n",
    "    Functions removes html tags, accented words,\n",
    "    and expands contractions(requires contraction dictionay,\n",
    "    also removes_special_characters. default setting false\n",
    "    '''\n",
    "    import nltk\n",
    "    from nltk.tokenize.toktok import ToktokTokenizer\n",
    "    import re\n",
    "    import spacy \n",
    "    from bs4 import BeautifulSoup\n",
    "    import unicodedata\n",
    "    from contractions import CONTRACTION_MAP \n",
    "    # this file is in usr/local/bin\n",
    "    # downloaded from https://github.com/dipanjanS\\\n",
    "    # /practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/contractions.py\n",
    "    import unicodedata\n",
    "    # DEFINE executables and list from modules \n",
    "    nlp = spacy.load('en', parse=True, \n",
    "                     tag=True, entity=True)\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    stopword_list.remove('no')\n",
    "    stopword_list.remove('not')\n",
    "    expansion_map=CONTRACTION_MAP\n",
    "    def expand_match(contraction):\n",
    "        #print('print this',contraction)\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = expansion_map.get(match) if expansion_map.get(match)\\\n",
    "        else expansion_map.get(match.lower())\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    \n",
    "    try: \n",
    "        normalized_corpus=[]\n",
    "        for doc in corpus[:5]:\n",
    "           # print(doc)\n",
    "            if strip_html:# strip HTML\n",
    "                soup = BeautifulSoup(doc, 'html.parser')\n",
    "                doc = soup.get_text()\n",
    "                \n",
    "            if accent:# Removing accented characters \n",
    "                doc=unicodedata.normalize('NFKD',doc).\\\n",
    "                    encode('ascii', 'ignore').\\\n",
    "                    decode('utf-8', 'ignore')\n",
    "            if special_char_removal:# remove special xter and numbers\n",
    "                special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "                doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "                if remove_digits:\n",
    "                    pattern = r'[^a-zA-z\\s]'\n",
    "                    doc = re.sub(pattern, '',doc)       \n",
    "                else:\n",
    "                    pattern = r'[^a-zA-z0-9\\s]'\n",
    "                    doc = re.sub(pattern, '',doc)\n",
    "            if expansion and expansion_map:# expand contractions\n",
    "                contraction_pattern = re.compile('({})'.\\\n",
    "                format('|'.join(expansion_map.keys())), \n",
    "                flags=re.IGNORECASE|re.DOTALL)\n",
    "                expanded_text = contraction_pattern.sub(expand_match,doc)\n",
    "                doc = re.sub(\"'\", \"\" , expanded_text)\n",
    "            if stemmer:# get word base form\n",
    "                ps=nltk.porter.PorterStemmer()\n",
    "                doc=' '.join([ps.stem(word) for word in doc.split()])\n",
    "            if lemmatize:# remove word affixes get word base form\n",
    "                text=nlp(doc)\n",
    "                doc=' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "                #print(doc)\n",
    "            if stopword:# remove words with little or no significance\n",
    "                tokens=tokenizer.tokenize(doc)\n",
    "                tokens=[token.strip() for token in tokens]\n",
    "                if lower_case:\n",
    "                    filtered_tokens=[token for token in tokens\\\n",
    "                                     if token not in stopword_list]\n",
    "                else:\n",
    "                    filtered_tokens = [token for token in tokens\n",
    "                                       if token.lower() not in stopword_list]\n",
    "                doc = ' '.join(filtered_tokens)  \n",
    "                # remove extra white space\n",
    "                doc = re.sub(' +', ' ',doc)\n",
    "                normalized_corpus.append(doc)\n",
    "        return normalized_corpus               \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "today=build_data(seed_urls)# scrape data \n",
    "#combine news headline with article\n",
    "today['full_text'] = today.news_headline.map(str) +\\\n",
    "  '. ' + today.news_article \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oppo launch new f pro mpmp dual rear camera allnew oppo f pro enable user capture low light moment ai driven mpmp rear camera It feature panoramic screen display vooc take flash charge technology step ahead smartphone also power p chipset faster smart robust performance',\n",
       " 'oneplus march madness campaign offer various deal oneplus oneplus announce march madness campaign part oneplus available several offer across amazon oneplus reliance digital outlet croma outlet oneplus offline store offer include nocost emi month discount exchange among various additionally offer special giveaway form accessory bundle',\n",
       " 'google find pay man less woman job google internal audit find company pay man work level software engineers less discretionary fund woman role google end pay million pay adjustment employee year include new employee however google not specify many employee man',\n",
       " 'indian govt website hack post jk attack report indian government website critical system hack within hour pulwama terror attack say official india resort offensive measure cyber domain counter attack attacker not able breach firewall protective measure critical system financial system power grid management one specially target',\n",
       " 'lego troll foldable smartphone foldable product danish company lego troll foldable phone twitter image foldable product lego fold caption endless creative play never run battery describe inch cover display [ ] unfold inch storybook lego fold legos popup storybook product last month company include samsung huawei unveil foldable phone']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            if stopword:# remove words with little or no significance\n",
    "                tokens=tokenizer.tokenize(doc)\n",
    "                tokens=[token.strip() for token in tokens]\n",
    "                if lower_case:\n",
    "                    filtered_tokens=[token for token in tokens\\\n",
    "                                     if token not in stopword_list]\n",
    "                else:\n",
    "                    filtered_tokens = [token for token in tokens\n",
    "                                       if token.lower() not in stopword_list]\n",
    "                doc = ' '.join(filtered_tokens)  \n",
    "            # remove extra white space\n",
    "                doc = re.sub(' +', ' ',doc)\n",
    "            \n",
    "                normalized_corpus.append(doc)\n",
    "            \n",
    "            return normalized_corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
